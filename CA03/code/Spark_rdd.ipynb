{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run this file on google colab!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1703875346671,
     "user": {
      "displayName": "Abolfazl Eslami",
      "userId": "03865273772694076131"
     },
     "user_tz": -210
    },
    "id": "LuwwQ181m_hd"
   },
   "outputs": [],
   "source": [
    "!apt-get update # Update apt-get repository.\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # Install Java.\n",
    "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz # Download Apache Sparks.\n",
    "!tar xf spark-3.1.1-bin-hadoop3.2.tgz # Unzip the tgz file.\n",
    "!pip install -q findspark # Install findspark. Adds PySpark to the System path during runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1703875346671,
     "user": {
      "displayName": "Abolfazl Eslami",
      "userId": "03865273772694076131"
     },
     "user_tz": -210
    },
    "id": "AQ1I4Qmlp5E2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, col, substring, split, size\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from string import punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 12484,
     "status": "ok",
     "timestamp": 1703875359151,
     "user": {
      "displayName": "Abolfazl Eslami",
      "userId": "03865273772694076131"
     },
     "user_tz": -210
    },
    "id": "r10LgahNp-cJ"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtZmGkloalIc"
   },
   "source": [
    "### Read text from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 3048,
     "status": "ok",
     "timestamp": 1703875362189,
     "user": {
      "displayName": "Abolfazl Eslami",
      "userId": "03865273772694076131"
     },
     "user_tz": -210
    },
    "id": "euZsBdtuGkay"
   },
   "outputs": [],
   "source": [
    "text_file_path = \"news.txt\"\n",
    "text_df = spark.read.text(text_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HADgtPrsGo6Q"
   },
   "source": [
    "### Number of news and words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-upYqQTobh29"
   },
   "source": [
    "Following cell performs the following operations using PySpark:\n",
    "\n",
    "1. **Counting Paragraphs:**\n",
    "   - `paragraphs_df = text_df.select(split(text_df.value, \"\\n\").alias(\"paragraphs\"))`: This line splits the text into paragraphs using newline (\"\\n\") as the delimiter and creates a DataFrame with a single column named \"paragraphs\" containing arrays of words.\n",
    "   - `paragraph_count_df = paragraphs_df.select(size(paragraphs_df.paragraphs).alias(\"paragraph_count\"))`: This line calculates the size (number of elements) of each array in the \"paragraphs\" column, essentially giving the count of paragraphs in each row.\n",
    "   - `total_paragraph_count = paragraph_count_df.agg({\"paragraph_count\": \"sum\"}).collect()[0][0]`: This line aggregates the counts to get the total number of paragraphs in the entire file.\n",
    "\n",
    "2. **Counting Words:**\n",
    "   - `words_df = text_df.select(split(text_df.value, \" \").alias(\"words\"))`: This line splits the text into words using space (\" \") as the delimiter and creates a DataFrame with a single column named \"words\" containing arrays of words.\n",
    "   - `word_count_df = words_df.select(size(words_df.words).alias(\"word_count\"))`: This line calculates the size (number of elements) of each array in the \"words\" column, essentially giving the count of words in each row.\n",
    "   - `total_word_count = word_count_df.agg({\"word_count\": \"sum\"}).collect()[0][0]`: This line aggregates the counts to get the total number of words in the entire file.\n",
    "\n",
    "3. **Getting the First Five Words from the First Row:**\n",
    "   - `first_row_words = words_df.head(1)[0][\"words\"][:5]`: This line retrieves the first row of the \"words\" column, which is an array of words, and then selects the first five words from that array.\n",
    "\n",
    "Finally, the code prints the total number of paragraphs, total number of words, and the first five words from the first row. It provides basic statistics and insights into the structure of the text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7347,
     "status": "ok",
     "timestamp": 1703875369526,
     "user": {
      "displayName": "Abolfazl Eslami",
      "userId": "03865273772694076131"
     },
     "user_tz": -210
    },
    "id": "jp7YU349nXaL",
    "outputId": "0a9edd12-42c0-419a-9bd9-8c3072e884b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of paragraphs in the file: 12\n",
      "Total number of words in the file: 2787\n",
      "First five words: ['JAPAN', 'TO', 'REVISE', 'LONG', '-']\n"
     ]
    }
   ],
   "source": [
    "paragraphs_df = text_df.select(split(text_df.value, \"\\n\").alias(\"paragraphs\"))\n",
    "paragraph_count_df = paragraphs_df.select(size(paragraphs_df.paragraphs).alias(\"paragraph_count\"))\n",
    "total_paragraph_count = paragraph_count_df.agg({\"paragraph_count\": \"sum\"}).collect()[0][0]\n",
    "print(\"Total number of paragraphs in the file:\", total_paragraph_count)\n",
    "\n",
    "words_df = text_df.select(split(text_df.value, \" \").alias(\"words\"))\n",
    "word_count_df = words_df.select(size(words_df.words).alias(\"word_count\"))\n",
    "total_word_count = word_count_df.agg({\"word_count\": \"sum\"}).collect()[0][0]\n",
    "print(\"Total number of words in the file:\", total_word_count)\n",
    "\n",
    "first_row_words = words_df.head(1)[0][\"words\"][:5]\n",
    "print(\"First five words:\", first_row_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LoToHbbYJ4l5"
   },
   "source": [
    "### Top ten most repeated words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-c12sXREb9CH"
   },
   "source": [
    "Next cell performs the following tasks:\n",
    "\n",
    "1. **Lowercasing Words:**\n",
    "   - Converts all words in the \"words\" column of the original DataFrame (`words_df`) to lowercase, creating a new DataFrame (`lowercase_words_df`).\n",
    "\n",
    "2. **Exploding the Array of Words:**\n",
    "   - Transforms each array of words into separate rows, resulting in a DataFrame (`exploded_words_df`) with a single column \"word\" containing individual words.\n",
    "\n",
    "3. **Counting Word Occurrences:**\n",
    "   - Groups the DataFrame by the \"word\" column and counts the occurrences of each word, creating a new DataFrame (`word_counts_df`) with columns \"word\" and \"count.\"\n",
    "\n",
    "4. **Selecting Top 10 Words:**\n",
    "   - Orders the DataFrame by word frequency in descending order and selects the top 10 words, creating a new DataFrame (`top_10_words`).\n",
    "\n",
    "5. **Displaying the Result:**\n",
    "   - Prints the top 10 words and their frequencies without truncation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7846,
     "status": "ok",
     "timestamp": 1703875377369,
     "user": {
      "displayName": "Abolfazl Eslami",
      "userId": "03865273772694076131"
     },
     "user_tz": -210
    },
    "id": "6ng5a7Ymsv8O",
    "outputId": "4b387957-c5e4-4609-de7c-b4ec4e1fe48d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "|.   |130  |\n",
      "|the |123  |\n",
      "|,   |102  |\n",
      "|to  |84   |\n",
      "|of  |64   |\n",
      "|said|55   |\n",
      "|and |55   |\n",
      "|in  |54   |\n",
      "|a   |45   |\n",
      "|s   |33   |\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lowercase_words_df = words_df.selectExpr(\"transform(words, word -> lower(word)) as words\")\n",
    "\n",
    "exploded_words_df = lowercase_words_df.select(explode(lowercase_words_df.words).alias(\"word\"))\n",
    "word_counts_df = exploded_words_df.groupBy(\"word\").count()\n",
    "top_10_words = word_counts_df.orderBy(\"count\", ascending=False).limit(10)\n",
    "top_10_words.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pHzjMnU_cVUv"
   },
   "source": [
    "### Top ten most repeated words without punctuation\n",
    "  Following cells defines a PySpark script for cleaning and analyzing a DataFrame of words. Here's explanation:\n",
    "\n",
    "  1. **`remove_punctuation` Function:**\n",
    "    - Defines a Python function `remove_punctuation` that takes a list of words, removes punctuation from each word, and excludes empty strings.\n",
    "    - Registers this function as a PySpark User-Defined Function (UDF) named `remove_punctuation_udf` with the return type of an array of strings.\n",
    "\n",
    "  2. **UDF Registration:**\n",
    "    - Registers the `remove_punctuation` UDF to be used in Spark SQL queries.\n",
    "\n",
    "  3. **Temporary View Creation:**\n",
    "    - Creates a temporary view named \"lowercase_words_view\" from the DataFrame `lowercase_words_df`. This allows you to refer to the DataFrame in Spark SQL queries.\n",
    "\n",
    "  4. **Spark SQL Query:**\n",
    "    - Executes a Spark SQL query to apply the registered UDF to the \"words\" column of the \"lowercase_words_view\" and creates a new DataFrame `clean_words_df` with the cleaned words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1703875377369,
     "user": {
      "displayName": "Abolfazl Eslami",
      "userId": "03865273772694076131"
     },
     "user_tz": -210
    },
    "id": "bbQaSk13VKg8"
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(words):\n",
    "    translator = str.maketrans(\"\", \"\", punctuation)\n",
    "    cleaned_words = [word.translate(translator) for word in words]\n",
    "    cleaned_words = [word for word in cleaned_words if word.strip()]\n",
    "    return cleaned_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6908,
     "status": "ok",
     "timestamp": 1703875384266,
     "user": {
      "displayName": "Abolfazl Eslami",
      "userId": "03865273772694076131"
     },
     "user_tz": -210
    },
    "id": "JsSmvAyn_Y_m",
    "outputId": "8de68837-c190-49b1-d1b1-20d72aa3edd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "|the |123  |\n",
      "|to  |84   |\n",
      "|of  |64   |\n",
      "|and |55   |\n",
      "|said|55   |\n",
      "|in  |54   |\n",
      "|a   |45   |\n",
      "|s   |33   |\n",
      "|on  |28   |\n",
      "|for |22   |\n",
      "+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.udf.register(\"remove_punctuation_udf\", remove_punctuation, ArrayType(StringType()))\n",
    "lowercase_words_df.createOrReplaceTempView(\"lowercase_words_view\")\n",
    "clean_words_df = spark.sql(\"SELECT remove_punctuation_udf(words) as words FROM lowercase_words_view\")\n",
    "\n",
    "exploded_words_df = clean_words_df.select(explode(clean_words_df.words).alias(\"word\"))\n",
    "word_counts_df = exploded_words_df.groupBy(\"word\").count()\n",
    "top_words = word_counts_df.orderBy(\"count\", ascending=False)\n",
    "top_words.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjJqtGzCcsuR"
   },
   "source": [
    " ### Top ten most repeated letters in first\n",
    " Following cell extends the analysis to calculate and display the count of words based on their first letters from the previously obtained `top_words` DataFrame:\n",
    "\n",
    "1. **Extracting First Letters:**\n",
    "   - Adds a new column \"first_letter\" to the `top_words` DataFrame, containing the first letter of each word.\n",
    "\n",
    "\n",
    "2. **Counting Words by First Letter:**\n",
    "   - Groups the DataFrame (`first_letter_df`) by the \"first_letter\" column.\n",
    "   - Aggregates the counts of each first letter, renaming the result column to \"letter_count.\"\n",
    "\n",
    "\n",
    "3. **Sorting Letter Counts:**\n",
    "   - Orders the DataFrame (`letter_counts`) by the \"letter_count\" column in descending order.\n",
    "\n",
    "4. **Displaying Top 5 Letter Counts:**\n",
    "   - Prints the top 5 letters along with the count of words starting with each letter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8675,
     "status": "ok",
     "timestamp": 1703875392931,
     "user": {
      "displayName": "Abolfazl Eslami",
      "userId": "03865273772694076131"
     },
     "user_tz": -210
    },
    "id": "2Lz6sCziMpd5",
    "outputId": "13dd44b6-166f-44a3-a8d9-d061c117b903"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|first_letter|letter_count|\n",
      "+------------+------------+\n",
      "|t           |337         |\n",
      "|a           |224         |\n",
      "|s           |200         |\n",
      "|o           |164         |\n",
      "|i           |150         |\n",
      "+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_letter_df = top_words.withColumn(\"first_letter\", substring(col(\"word\"), 1, 1))\n",
    "letter_counts = first_letter_df.groupBy(\"first_letter\").agg({\"count\": \"sum\"}).withColumnRenamed(\"sum(count)\", \"letter_count\")\n",
    "sorted_letter_counts = letter_counts.orderBy(\"letter_count\", ascending=False)\n",
    "sorted_letter_counts.show(5, truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMf3wMMB0EKXslKRPrfd2mT",
   "collapsed_sections": [
    "HADgtPrsGo6Q",
    "LoToHbbYJ4l5",
    "pHzjMnU_cVUv",
    "EjJqtGzCcsuR"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
