{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run this file on google colab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1703878809824,
     "user": {
      "displayName": "Abolfazl Eslami",
      "userId": "03865273772694076131"
     },
     "user_tz": -210
    },
    "id": "dHkj865A1nLj"
   },
   "outputs": [],
   "source": [
    "# !apt-get update # Update apt-get repository.\n",
    "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null # Install Java.\n",
    "# !wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz # Download Apache Sparks.\n",
    "# !tar xf spark-3.1.1-bin-hadoop3.2.tgz # Unzip the tgz file.\n",
    "# !pip install -q findspark # Install findspark. Adds PySpark to the System path during runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 351,
     "status": "ok",
     "timestamp": 1703878810171,
     "user": {
      "displayName": "Abolfazl Eslami",
      "userId": "03865273772694076131"
     },
     "user_tz": -210
    },
    "id": "LUSCmgQS2Am-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, col, substring, split, size, log, count, \\\n",
    "                                  countDistinct, monotonically_increasing_id\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 9806,
     "status": "ok",
     "timestamp": 1703878819976,
     "user": {
      "displayName": "Abolfazl Eslami",
      "userId": "03865273772694076131"
     },
     "user_tz": -210
    },
    "id": "2_qBWlFO2Pfa"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62AI-dHG2eOn"
   },
   "source": [
    "### Read text from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 4049,
     "status": "ok",
     "timestamp": 1703878824015,
     "user": {
      "displayName": "Abolfazl Eslami",
      "userId": "03865273772694076131"
     },
     "user_tz": -210
    },
    "id": "wXqYmF582UG_"
   },
   "outputs": [],
   "source": [
    "text_file_path = \"news.txt\"\n",
    "text_df = spark.read.text(text_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PuUGn-iVGj4r"
   },
   "source": [
    "Following cell defines a function called `remove_punctuation` that takes a list of words as input and removes punctuation from each word. The function uses the `str.maketrans` method to create a translation table that maps each punctuation character to `None`. It then applies this translation to each word using a list comprehension, effectively removing punctuation.\n",
    "\n",
    "The function also includes a second list comprehension to filter out any empty strings that might result from the removal of punctuation. The cleaned words are returned as a list.\n",
    "\n",
    "The code then registers this Python function as a User Defined Function (UDF) in Spark with the name `remove_punctuation_udf` using `spark.udf.register`. This UDF can then be used in Spark SQL queries to apply the punctuation removal logic to DataFrame columns. Note that `ArrayType(StringType())` specifies the return type of the UDF as a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1703878824015,
     "user": {
      "displayName": "Abolfazl Eslami",
      "userId": "03865273772694076131"
     },
     "user_tz": -210
    },
    "id": "B6ua0Hea-QrX",
    "outputId": "95391f43-ac21-43c5-bd06-a0a13deecef0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.remove_punctuation(words)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punctuation(words):\n",
    "    translator = str.maketrans(\"\", \"\", punctuation)\n",
    "    cleaned_words = [word.translate(translator) for word in words]\n",
    "    cleaned_words = [word for word in cleaned_words if word.strip()]\n",
    "    return cleaned_words\n",
    "\n",
    "spark.udf.register(\"remove_punctuation_udf\", remove_punctuation, ArrayType(StringType()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGw7N6lRG2ta"
   },
   "source": [
    "This following cell processes a DataFrame (`text_df`) containing text data.\n",
    "\n",
    "1. **Splitting into Words:**\n",
    "   Splits the text in the \"value\" column of `text_df` into words, and the result is stored in a new DataFrame (`row_df`) with a column named \"words.\"\n",
    "\n",
    "2. **Lowercasing:**\n",
    "   Converts all the words to lowercase using the `lower` function and creates a new DataFrame (`lowercase_lines_df`) with the resulting lowercase words.\n",
    "\n",
    "3. **Exploding Words:**\n",
    "   Uses the `explode` function to transform the array of words into separate rows for each word. The result is a DataFrame (`exploded_lines_df`) with a column named \"word.\"\n",
    "\n",
    "4. **Creating a Temporary View:**\n",
    "   Creates a temporary view named \"lowercase_words_view\" from the DataFrame `lowercase_lines_df`. This allows you to use Spark SQL queries on this view.\n",
    "\n",
    "5. **Applying Punctuation Removal UDF:**\n",
    "   Uses a Spark SQL query to apply the previously registered UDF (`remove_punctuation_udf`) to remove punctuation from the words in the \"lowercase_words_view\" view. The result is a DataFrame (`clean_lines_df`) with a column named \"words\" containing cleaned words.\n",
    "\n",
    "6. **Assigning Document IDs:**\n",
    "   Adds a new column \"doc_id\" to the DataFrame `clean_lines_df` using the `monotonically_increasing_id` function. This column is assigned a unique identifier for each row, essentially serving as a document ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 1605,
     "status": "ok",
     "timestamp": 1703878825610,
     "user": {
      "displayName": "Abolfazl Eslami",
      "userId": "03865273772694076131"
     },
     "user_tz": -210
    },
    "id": "sbK_c1HZ3VBG"
   },
   "outputs": [],
   "source": [
    "row_df = text_df.select(split(text_df.value, \" \").alias(\"words\"))\n",
    "lowercase_lines_df = row_df.selectExpr(\"transform(words, word -> lower(word)) as words\")\n",
    "exploded_lines_df = lowercase_lines_df.select(explode(lowercase_lines_df.words).alias(\"word\"))\n",
    "lowercase_lines_df.createOrReplaceTempView(\"lowercase_words_view\")\n",
    "clean_lines_df = spark.sql(\"SELECT remove_punctuation_udf(words) as words FROM lowercase_words_view\")\n",
    "lines_df = clean_lines_df.withColumn(\"doc_id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hT2Hd3M7Ha9r"
   },
   "source": [
    "This following cell calculates the TF-IDF (Term Frequency-Inverse Document Frequency) values for each word in a collection of documents.\n",
    "\n",
    "1. **Calculate Term Frequency (TF):**\n",
    "   This step explodes the array of words into separate rows for each word, groups by document ID and word, and calculates the term frequency (`word_count`) for each word in each document.\n",
    "\n",
    "2. **Calculate Total Word Count per Document:**\n",
    "   Calculates the total word count per document by summing the word counts. The result is stored in a DataFrame (`total_word_count_df`).\n",
    "\n",
    "3. **Calculate TF (Normalized Term Frequency):**\n",
    "   Joins the TF DataFrame with the total word count DataFrame, calculates the normalized TF, and drops unnecessary columns.\n",
    "\n",
    "4. **Calculate Document Frequency (DF):**\n",
    "   Groups by word and calculates the document frequency (number of documents where each word appears).\n",
    "\n",
    "5. **Calculate Inverse Document Frequency (IDF):**\n",
    "   Calculates the inverse document frequency (IDF) for each word.\n",
    "\n",
    "6. **Calculate TF-IDF:**\n",
    "   Joins the TF and IDF DataFrames, selects relevant columns, and calculates the TF-IDF values for each word in each document.\n",
    "\n",
    "7. **Show Results:**\n",
    "   Displays the first 10 rows of the resulting TF-IDF DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31636,
     "status": "ok",
     "timestamp": 1703878857244,
     "user": {
      "displayName": "Abolfazl Eslami",
      "userId": "03865273772694076131"
     },
     "user_tz": -210
    },
    "id": "WRNnqfvl5k1N",
    "outputId": "b8e7c368-7b76-4f46-9616-dfe67b671506"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------------+------------------+---------------------+\n",
      "|doc_id|word    |tf                   |idf               |tf_idf               |\n",
      "+------+--------+---------------------+------------------+---------------------+\n",
      "|6     |priority|0.0035714285714285713|1.791759469228055 |0.006399140961528767 |\n",
      "|3     |some    |0.002531645569620253 |1.791759469228055 |0.004536099922096342 |\n",
      "|3     |still   |0.002531645569620253 |1.0986122886681096|0.002781296933336986 |\n",
      "|2     |still   |0.004878048780487805 |1.0986122886681096|0.005359084334966388 |\n",
      "|9     |still   |0.005025125628140704 |1.0986122886681096|0.00552066476717643  |\n",
      "|11    |tonnes  |0.005263157894736842 |1.3862943611198906|0.0072962861111573185|\n",
      "|3     |tonnes  |0.005063291139240506 |1.3862943611198906|0.007019211955037421 |\n",
      "|5     |import  |0.010752688172043012 |1.0986122886681096|0.011813035362022684 |\n",
      "|1     |import  |0.006289308176100629 |1.0986122886681096|0.006909511249484966 |\n",
      "|6     |import  |0.0035714285714285713|1.0986122886681096|0.00392361531667182  |\n",
      "+------+--------+---------------------+------------------+---------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_df = lines_df.select(\"doc_id\", explode(\"words\").alias(\"word\")) \\\n",
    "    .groupBy(\"doc_id\", \"word\").agg(count(\"*\").alias(\"word_count\"))\n",
    "\n",
    "total_word_count_df = tf_df.groupBy(\"doc_id\").agg({\"word_count\": \"sum\"}).withColumnRenamed(\"sum(word_count)\", \"total_word_count\")\n",
    "\n",
    "tf_df = tf_df.join(total_word_count_df, \"doc_id\") \\\n",
    "    .withColumn(\"tf\", col(\"word_count\") / col(\"total_word_count\")) \\\n",
    "    .drop(\"word_count\", \"total_word_count\")\n",
    "\n",
    "df_df = tf_df.groupBy(\"word\").agg(countDistinct(\"doc_id\").alias(\"document_frequency\"))\n",
    "\n",
    "total_docs = lines_df.select(\"doc_id\").distinct().count()\n",
    "idf_df = df_df.withColumn(\"idf\", log(total_docs / (col(\"document_frequency\") + 1)))\n",
    "\n",
    "tf_idf_df = tf_df.join(idf_df, \"word\").select(\"doc_id\", \"word\", \"tf\", \"idf\") \\\n",
    "      .withColumn(\"tf_idf\", col(\"tf\") * col(\"idf\"))\n",
    "tf_idf_df.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VTg5rc1Hrke"
   },
   "source": [
    "### results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16960,
     "status": "ok",
     "timestamp": 1703878874193,
     "user": {
      "displayName": "Abolfazl Eslami",
      "userId": "03865273772694076131"
     },
     "user_tz": -210
    },
    "id": "yA294_BG9oti",
    "outputId": "ff53e869-cf7b-415b-9eed-c2ed5bd55514"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+--------------------+------------------+--------------------+\n",
      "|doc_id|word|tf                  |idf               |tf_idf              |\n",
      "+------+----+--------------------+------------------+--------------------+\n",
      "|0     |gas |0.0111731843575419  |1.3862943611198906|0.015489322470613303|\n",
      "|4     |gas |0.009259259259259259|1.3862943611198906|0.012836058899258245|\n",
      "+------+----+--------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = tf_idf_df.filter(col(\"word\") == \"gas\")\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16468,
     "status": "ok",
     "timestamp": 1703878890650,
     "user": {
      "displayName": "Abolfazl Eslami",
      "userId": "03865273772694076131"
     },
     "user_tz": -210
    },
    "id": "eBXd_y5f9pCz",
    "outputId": "e0cdc97e-02cf-4524-9705-11cac2ed9bfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------------+------------------+--------------------+\n",
      "|doc_id|word |tf                  |idf               |tf_idf              |\n",
      "+------+-----+--------------------+------------------+--------------------+\n",
      "|6     |japan|0.03214285714285714 |0.8754687373538999|0.02814006655780392 |\n",
      "|0     |japan|0.0111731843575419  |0.8754687373538999|0.009781773601719551|\n",
      "|7     |japan|0.021739130434782608|0.8754687373538999|0.019031929072910864|\n",
      "|5     |japan|0.03225806451612903 |0.8754687373538999|0.028240927011416124|\n",
      "+------+-----+--------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = tf_idf_df.filter(col(\"word\") == \"japan\")\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16460,
     "status": "ok",
     "timestamp": 1703878907099,
     "user": {
      "displayName": "Abolfazl Eslami",
      "userId": "03865273772694076131"
     },
     "user_tz": -210
    },
    "id": "y760fdHZDFUQ",
    "outputId": "65442b4e-5528-4fe5-edd6-cc1cc88a4211"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+--------------------+------------------+--------------------+\n",
      "|doc_id|word  |tf                  |idf               |tf_idf              |\n",
      "+------+------+--------------------+------------------+--------------------+\n",
      "|10    |market|0.00423728813559322 |0.8754687373538999|0.003709613293872457|\n",
      "|11    |market|0.010526315789473684|0.8754687373538999|0.009215460393198946|\n",
      "|7     |market|0.021739130434782608|0.8754687373538999|0.019031929072910864|\n",
      "|6     |market|0.010714285714285714|0.8754687373538999|0.009380022185934641|\n",
      "+------+------+--------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = tf_idf_df.filter(col(\"word\") == \"market\")\n",
    "result.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOczi5cMtyjnSeB9/6wVvqG",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
